import jax
from jax import numpy as jnp
from jax import random
from jax import nn

def attention_layer(key_dim, value_dim, model_dim, name, batch_size, key = 0):
    key = random.PRNGKey(key)
    ## W_Q is d_model x d_key
    W_Q = random.normal(key, (model_dim, key_dim))
    ## W_K is d_model x d_key
    W_K = random.normal(key, (model_dim, key_dim))
    ## W_V is d_model x d_value
    W_V = random.normal(key, (model_dim, value_dim))
    ## q, k and v are b x n x d_model
    def attention(q, k, v):
        return nn.softmax(((q@W_Q)@(k@W_K).transpose(axes=(0,2,1)), axis=0)@(v@W_V)

    params = {name: dict(W_Q = W_Q, W_K = W_K, W_V = W_V)}
    return attention, params

def attention_layer_smoke_test():
    key_dim = 2
    value_dim = 4
    model_dim = 3
    name = 'attention'
    key = random.PRNGKey(0)
    x = attention_layer(key_dim, value_dim, model_dim, name)
    q = random.normal(key, (5, model_dim))
    k = random.normal(key, (5, model_dim))
    v = random.normal(key, (5, model_dim))

    att = x[0](q,k,v)

    print(att)
    print(x[1])

def attention_take_apart():
    ## q, k, v are batch size x max input seq length x model dimension
    model_dim = 3
    batch_size = 2
    max_input_length = 4
                          
