#+TITLE: Attention From Scratch

This is a literate programming documentat about implementation of a simple transformer as described in "Attention is all you need" paper.

* Basics of attention
In context of sequential data, attention allows neural networks to communicate about connection between various input tokens and input and output tokens. As an example, consider the details of attention in the encoder blocks which is a bit simpler to understand but amply highlights the communication aspect of attention blocks.

** Details of a single attention head
*** An abstract overview
Presentation in this section is quite abstract. Please bear with me for a moment. This abstraction allows us to understand the essence of attention without getting bogged down in the details. In the following sections, we will connect these abstract entities to concrete representations and implementation.

An attention head has as its inputs, queries, keys and values with 1-1 correspondence with each other. In effect, we can consider the input to be a sequence of n  (query, key, value) tuples, where n is the length of input sequence. Only thing we need to know for now is that there exists a notion of distance between any pair of queries and keys. Thus, (query, key) tuples induce a directed graph over  the n input tokens where the distance between Token i and Token j is the distance between query_i and key_j. In this graph, each node (corresponding to each input token) has an associated value. The output for an attention head is updated values. These updated values are based on how close (or far) another node is. At each node, the updated value is a convex combination of values of all the other nodes. The weight assigned to value_j, the value for node j, when computing updated value for node_i depends on closeness of nodes i and j, as determined by distance between query_i and key_j. This can be thought of as sort of message passing over a directed graph.
*** Concretization
Now I will try to explain how queries, keys are values are represented and how this approach can be efficiently implemented on a GPU/TPU.
For now, we will focus on a single attention head and assume queries, keys and values as a given. We will discuss generalization of batched multi-head attention later and there I will clarify where the queries, keys and values come from.
**** Representation of queries, keys and values
Queries and keys are represented as k dimensional vectors. For n input tokens, queries and keys are n x k dimensional matrices. Similarly, values are n x v dimensional matrices.
**** Computation of distance (closeness)
Distance (rather closeness) between queries and keys is simply computed as a dot product. Thus, the closeness of node i to node j is given by query_i @ key_j.transpose() where @ is matrix multiplication. Distance over all pairs of queries and keys can simply be written as Q@K.transpose() where Q and K are n x k dimensional input queries and keys, respectively. Variance of dot product increases with dimension of input. Therefore, the dot product is often normalized with \sqrt(k) to prevent variance from increasing too much. Even after this normalization, the dot products are not well bounded and can take arbitrary values.

Output is an n x n attention matrix that is often used to generate cool attention visualizations which may or may not be interpretable. However, these 

