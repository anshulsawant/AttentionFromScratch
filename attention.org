#+TITLE: Attention From Scratch

This is a literate programming documentat about implementation of a simple transformer as described in "Attention is all you need" paper.

* Basics of attention
In context of sequential data, attention allows neural networks to communicate about connection between various input tokens and input and output tokens. As an example, consider the details of attention in the encoder blocks which is a bit simpler to understand but amply highlights the communication aspect of attention blocks.

** Details of a single attention head

Presentation in this section is quite abstract. Please bear with me for a moment. This abstraction allows us to understand the essence of attention without getting bogged down in the details. In the following sections, we will connect these abstract entities to concrete representations and implementation.

An attention head has as its inputs, queries, keys and values with 1-1 correspondence with each other. In effect, we can consider the input to be a sequence of n  (query, key, value) tuples, where n is the length of input sequence. Only thing we need to know for now is that there exists a notion of distance between any pair of queries and keys. Thus, (query, key) tuples induce a directed graph over  the n input tokens where the distance between Token i and Token j is the distance between query_i and key_j. In this graph, each node (corresponding to each input token) has an associated value.
